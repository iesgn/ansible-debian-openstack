[DEFAULT]

### nova.availability_zones ###
###############################
# availability_zone to show internal services under (string value)
#internal_service_availability_zone=internal

# default compute node availability_zone (string value)
#default_availability_zone=nova

### nova.crypto ###
###################
# Filename of root CA (string value)
#ca_file=cacert.pem

# Filename of private key (string value)
#key_file=private/cakey.pem

# Filename of root Certificate Revocation List (string value)
#crl_file=crl.pem

# Where we keep our keys (string value)
#keys_path=$state_path/keys
# Where we keep our root CA (string value)
#ca_path=$state_path/CA

# Should we use a CA for each project? (boolean value)
#use_project_ca=false

# Subject for certificate for users, %s for project, user,
# timestamp (string value)
#user_cert_subject=/C=US/ST=California/O=OpenStack/OU=NovaDev/CN=%.16s-%.16s-%s

# Subject for certificate for projects, %s for project,
# timestamp (string value)
#project_cert_subject=/C=US/ST=California/O=OpenStack/OU=NovaDev/CN=project-ca-%.16s-%s

### nova.exception ###
# make exception message format errors fatal (boolean value)
#fatal_exception_format_errors=false

### nova.manager ###
# Some periodic tasks can be run in a separate process. Should
# we run them here? (boolean value)
#run_external_periodic_tasks=true

############
# DATABASE #
############
sql_connection=mysql://nova:{{ nova_db_password }}@{{ controller_ip }}/nova

###########
# COMPUTE #
###########
# Use qemu because we're running insode a VM                                      
compute_driver=nova.virt.libvirt.LibvirtDriver
libvirt_type=qemu

########
# APIs #
########
# Selects the type of APIs you want to activate.
# Each API will bind on a specific port.
# Compute nodes should run only the metadata API,
# a nova API endpoint node should run osapi_compute.
# If you want to use nova-volume you can also enable
# osapi_volume, but if you want to run cinder, do not
# activate it.
# The list of API is: ec2,osapi_compute,metadata,osapi_volume
enabled_apis=ec2,osapi_compute,metadata

# NOVA API #
# # # # #  #
osapi_compute_listen="0.0.0.0"
#osapi_compute_listen_port=8774
allow_admin_api=true
api_paste_config=/etc/nova/api-paste.ini
nova_url=http://{{ controller_ip }}:8774/v1.1/
#osapi_path="/v1.1/"
# Allows use of instance password during server creation
#enable_instance_password=true
my_ip = {{ controller_ip }}
# Name of this node. This can be an opaque identifier. It is not necessarily a hostname, FQDN, or IP address.
#host="firefly-2.local"
#cc_host=192.168.0.1
# availability zone of this node
#node_availability_zone="nova"
# Nova API extentions:
#osapi_compute_extension=nova.api.openstack.compute.contrib.standard_extensions
#osapi_compute_ext_list=""
#allow_instance_snapshots=true

# S3 #
# #  #
#s3_host="{{ controller_ip }}"
#s3_dmz="{{ controller_ip }}"
#s3_port=3333

# EC2 API #
# # # # # #
ec2_host={{ controller_ip }}
ec2_dmz_host={{ controller_ip }}
#ec2_private_dns_show_ip=True
#ec2_path="/services/Cloud"
ec2_port=8773
ec2_scheme="http"
ec2_listen="0.0.0.0"
ec2_listen_port=8773

# Metadata API #
# # # # # # #  #
metadata_host={{ controller_ip }}
metadata_port=8775
metadata_listen_port=8775
metadata_manager=nova.api.manager.MetadataManager
metadata_listen=0.0.0.0

########
# MISC #
########
#resume_guests_state_on_host_boot=false
#start_guests_on_host_boot=false
instance_name_template="instance-%08x"
# Inject the admin password at boot time, without an agent.
#libvirt_inject_password=false

########
# LOGS #
########
logdir=/var/log/nova
#log-date-format="%Y-%m-%d %H:%M:%S"
debug={{ log_debug }}
#logfile_mode="0644"

##########
# SYSTEM #
##########
state_path=/var/lib/nova
lock_path=/var/lock/nova
rootwrap_config=/etc/nova/rootwrap.conf
#memcached_servers=<None>

##################
# AUTHENTICATION #
##################
auth_strategy=keystone
use_deprecated_auth=false
# Seconds for auth tokens to linger
# auth_token_ttl=3600

#############
# SCHEDULER #
#############
scheduler_driver=nova.scheduler.filter_scheduler.FilterScheduler

####################
# VOLUMES / CINDER #
####################
# iscsi_helper default is ietadm, but probably you want to use tgtadm
iscsi_helper=ietadm
# You need this if you want to use Cinder (now the default).
volume_api_class=nova.volume.cinder.API

# Allow to perform insecure SSL requests to cinder (boolean value)
#cinder_api_insecure=false

# Allow attach between instance and volume in different
# availability zones. (boolean value)
#cinder_cross_az_attach=true

# Libvirt handlers for remote volumes. (list value)
#libvirt_volume_drivers=iscsi=nova.virt.libvirt.volume.LibvirtISCSIVolumeDriver,local=nova.virt.libvirt.volume.LibvirtVolumeDriver,fake=nova.virt.libvirt.volume.LibvirtFakeVolumeDriver,rbd=nova.virt.libvirt.volume.LibvirtNetVolumeDriver,sheepdog=nova.virt.libvirt.volume.LibvirtNetVolumeDriver,nfs=nova.virt.libvirt.volume.LibvirtNFSVolumeDriver,aoe=nova.virt.libvirt.volume.LibvirtAOEVolumeDriver,glusterfs=nova.virt.libvirt.volume.LibvirtGlusterfsVolumeDriver,fibre_channel=nova.virt.libvirt.volume.LibvirtFibreChannelVolumeDriver,scality=nova.virt.libvirt.volume.LibvirtScalityVolumeDriver

############
# RABBITMQ #
############
rabbit_host={{ controller_ip }}
#fake_rabbit=false
#rabbit_virtual_host="/"
rabbit_userid="{{ rabbit_user }}"
rabbit_password="{{ rabbit_password }}"
#rabbit_port=5672
#rabbit_use_ssl=false
#rabbit_retry_interval=1
# The messaging module to use, defaults to kombu (works for rabbit).
# You can also use qpid: nova.rpc.impl_qpid
rpc_backend="nova.rpc.impl_kombu"

##########
# GLANCE #
##########
image_service=nova.image.glance.GlanceImageService
# Cache glance images locally
#cache_images=true
#glance_host="{{ controller_ip }}"
#glance_num_retries=0
glance_port=9292
glance_api_servers="{{ storage_ip }}:$glance_port"

#######################
# NETWORK (linux net) #
#######################
# Uncomment this to enable nova-network API
#network_api_class="nova.network.api.API" 
#network_manager=nova.network.manager.VlanManager
#multi_host=true
# should we use fake network devices and addresses 
#fake_network=false 
#force_dhcp_release=True
#dhcpbridge_flagfile=/etc/nova/nova.conf
#dhcpbridge=/usr/bin/nova-dhcpbridge
#dhcp_lease_time=120
#firewall_driver=nova.virt.libvirt.firewall.IptablesFirewallDriver
#public_interface=br-ext
#vlan_interface=eth1
#flat_network_bridge=br100
#flat_interface=eth0
#fixed_range=192.168.100.0/24
#use_ipv6=false
# set it to the /32 of your metadata server if you have just one
# It is a cidr in case there are multiple services that you want
# to keep using the internal private ips.
#dmz_cidr=169.254.169.254/32
#linuxnet_ovs_integration_bridge="br-int"
#routing_source_ip="{{ controller_ip }}"
# Only first nic of vm will get default gateway from dhcp server
#use_single_default_gateway=false

###########
# Quantum #
###########
# Quantum networking: uncomment, configure, and restart nova-api and nova-compute
# to use Quantum as your network manager
network_api_class=nova.network.quantumv2.api.API
security_group_api=quantum
# This is the URL of your quantum server:
quantum_url=http://{{ controller_ip }}:9696
quantum_auth_strategy=keystone
quantum_admin_tenant_name=service
quantum_admin_username=quantum
quantum_admin_password=quantum
quantum_admin_auth_url=http://{{ controller_ip }}:35357/v2.0
# Quantum metadata proxy
service_quantum_metadata_proxy = True
quantum_metadata_proxy_shared_secret = {{ shared_secret }}

# What's below is only needed for nova-compute.

# When using OVS with Nova security filtering, you should set this:
#libvirt_vif_driver=nova.virt.libvirt.vif.LibvirtHybirdOVSBridgeDriver
# This one is if you use the older security groups instead of security filtering:
libvirt_vif_driver = nova.virt.libvirt.vif.LibvirtOpenVswitchVirtualPortDriver

linuxnet_interface_driver=nova.network.linux_net.LinuxOVSInterfaceDriver
#firewall_driver=nova.virt.libvirt.firewall.IptablesFirewallDriver
firewall_driver=nova.virt.firewall.NoopFirewallDriver
libvirt_use_virtio_for_bridges=True

#################
# NOVNC CONSOLE #
#################
vnc_enabled=false
#novnc_enable=true
#novncproxy_base_url=http://{{ controller_ip  }}:6080/vnc_auto.html
# Change vncserver_proxyclient_address and vncserver_listen to match each compute host
#vncserver_proxyclient_address={{ controller_ip }}
#vncserver_listen={{ controller_ip }}
#vnc_keymap="en-us"

# xvpvncproxy #
# # # # # # # #
#xvpvncproxy_host="0.0.0.0"
#xvpvncproxy_port=6081

#########
# QUOTA #
#########
# number of instances allowed per project (integer value)
#quota_instances=10
# number of instance cores allowed per project (integer value)
#quota_cores=20
# megabytes of instance ram allowed per project (integer value)
#quota_ram=51200
# number of floating ips allowed per project (integer value)
#quota_floating_ips=10
# number of metadata items allowed per instance (integer value)
#quota_metadata_items=128
# number of injected files allowed (integer value)
#quota_injected_files=5
# number of bytes allowed per injected file (integer value)
#quota_injected_file_content_bytes=10240
# number of bytes allowed per injected file path (integer value)
#quota_injected_file_path_bytes=255
# number of security groups per project (integer value)
#quota_security_groups=10
# number of security rules per security group (integer value)
#quota_security_group_rules=20
# number of key pairs per user (integer value)
#quota_key_pairs=100
# number of seconds until a reservation expires (integer value)
#reservation_expire=86400
# count of reservations until usage is refreshed (integer value)
#until_refresh=0
# number of seconds between subsequent usage refreshes (integer value)
#max_age=0
# default driver to use for quota checks (string value)
#quota_driver=nova.quota.DbQuotaDriver

#############
# CONDUCTOR #
#############
[conductor]
# Perform nova-conductor operations locally (boolean value)
#use_local=false
# the topic conductor nodes listen on (string value)
#topic=conductor
# full class name for the Manager for conductor (string value)
#manager=nova.conductor.manager.ConductorManager

#########
# CELLS #
#########
[cells]
# Cells communication driver to use (string value)
#driver=nova.cells.rpc_driver.CellsRPCDriver

# Number of seconds after an instance was updated or deleted
# to continue to update cells (integer value)
#instance_updated_at_threshold=3600

# Number of instances to update per periodic task run (integer
# value)
#instance_update_num_instances=1

# Maximum number of hops for cells routing. (integer value)
#max_hop_count=10

# Cells scheduler to use (string value)
#scheduler=nova.cells.scheduler.CellsScheduler

# Enable cell functionality (boolean value)
#enable=false

# the topic cells nodes listen on (string value)
#topic=cells

# Manager for cells (string value)
#manager=nova.cells.manager.CellsManager

# name of this cell (string value)
#name=nova

# Key/Multi-value list with the capabilities of the cell (list
# value)
#capabilities=hypervisor=xenserver;kvm,os=linux;windows

# Seconds to wait for response from a call to a cell. (integer
# value)
#call_timeout=60

# Base queue name to use when communicating between cells.
# Various topics by message type will be appended to this.
# (string value)
#rpc_driver_queue_base=cells.intercell
 
# How many retries when no cells are available. (integer value)
#scheduler_retries=10

# How often to retry in seconds when no cells are available. (integer value)
#scheduler_retry_delay=2

# Seconds between getting fresh cell info from db. (integer value)
#db_check_interval=60

#############
# BAREMETAL #
#############
[baremetal]
# The backend to use for bare-metal database (string value)
#db_backend=sqlalchemy

# The SQLAlchemy connection string used to connect to the
# bare-metal database (string value)
#sql_connection=sqlite:///$state_path/baremetal_$sqlite_db

# Whether baremetal compute injects password or not (boolean value)
#inject_password=true

# Template file for injected network (string value)
#injected_network_template=$pybasedir/nova/virt/baremetal/interfaces.template

# Baremetal VIF driver. (string value)
#vif_driver=nova.virt.baremetal.vif_driver.BareMetalVIFDriver

# Baremetal volume driver. (string value)
#volume_driver=nova.virt.baremetal.volume_driver.LibvirtVolumeDriver

# a list of additional capabilities corresponding to
# instance_type_extra_specs for this compute host to
# advertise. Valid entries are name=value, pairs For example,
# "key1:val1, key2:val2" (list value)
#instance_type_extra_specs=

# Baremetal driver back-end (pxe or tilera) (string value)
#driver=nova.virt.baremetal.pxe.PXE

# Baremetal power management method (string value)
#power_manager=nova.virt.baremetal.ipmi.IPMI

# Baremetal compute node's tftp root path (string value)
#tftp_root=/tftpboot

# path to baremetal terminal program (string value)
#terminal=shellinaboxd

# path to baremetal terminal SSL cert(PEM) (string value)
#terminal_cert_dir=<None>

# path to directory stores pidfiles of baremetal_terminal
# (string value)
#terminal_pid_dir=$state_path/baremetal/console

# maximal number of retries for IPMI operations (integer
# value)
#ipmi_power_retry=5

# Default kernel image ID used in deployment phase (string
# value)
#deploy_kernel=<None>

# Default ramdisk image ID used in deployment phase (string
# value)
#deploy_ramdisk=<None>

# Template file for injected network config (string value)
#net_config_template=$pybasedir/nova/virt/baremetal/net-dhcp.ubuntu.template

# additional append parameters for baremetal PXE boot (string
# value)
#pxe_append_params=<None>

# Template file for PXE configuration (string value)
#pxe_config_template=$pybasedir/nova/virt/baremetal/pxe_config.template

# Timeout for PXE deployments. Default: 0 (unlimited) (integer
# value)
#pxe_deploy_timeout=0

# ip or name to virtual power host (string value)
#virtual_power_ssh_host=

# base command to use for virtual power(vbox,virsh) (string
# value)
#virtual_power_type=vbox

# user to execute virtual power commands as (string value)
#virtual_power_host_user=

# password for virtual power host_user (string value)
#virtual_power_host_pass=

# Do not set this out of dev/test environments. If a node does
# not have a fixed PXE IP address, volumes are exported with
# globally opened ACL (boolean value)
#use_unsafe_iscsi=false

# iSCSI IQN prefix used in baremetal volume connections.
# (string value)
#iscsi_iqn_prefix=iqn.2010-10.org.openstack.baremetal

##########
# VMWARE #
##########
[vmware]
# Name of Integration Bridge (string value)
#integration_bridge=br-int

#########
# SPICE #
#########
[spice]
# location of spice html5 console proxy, in the form
# "http://www.example.com:6082/spice_auto.html" (string value)
html5proxy_base_url=http://{{ controller_ip }}:6082/spice_auto.html

# IP address on which instance spice server should listen (string value)
server_listen=0.0.0.0

# the address to which proxy clients (like nova-spicehtml5proxy) should connect (string value)
server_proxyclient_address={{ controller_ip }}

# enable spice related features (boolean value)
enabled=true

# enable spice guest agent support (boolean value)
agent_enabled=true

# keymap for spice (string value)
keymap=en-us

##################
# AUTHENTICATION #
##################
auth_strategy=keystone
[keystone_authtoken]
auth_host = {{ controller_ip }}
auth_port = 35357
auth_protocol = http
admin_tenant_name = service
admin_user = nova
admin_password = {{ nova_identity_password }}
signing_dirname = /var/lib/nova/keystone-signing
